# Gradient Descent Exponent

## Description of constant

Given a convex function $f$ with black-box access to the function and its gradient, gradient descent will converge to a global minimum with an appropriate choice of _step size_ $s$: $x_{i+1} := x_i - s\cdot \nabla f(x_i)$. In general, $s$ can be chosen to vary with the step $i$. This will converge like $O(n^{-c})$ for some exponent $c$. It turns out that complicated patterns of increasing and decreasing step sizes can lead to improved rates of convergence, increasing the exponent.

The constant $C_{35}$ is the supremum of exponents $c$ such that there exists a step schedule $s : \mathbb{N} \to \mathbb{R}$ so that vanilla gradient descent has a worst-case convergence of $O(n^{-c})$.

When comparing to references, be aware that sometimes convergence is written as $O(\epsilon^{-c})$ to describe the number of steps as a function of accuracy; these are related by a reciprocal, and would be minimized then.

## Known upper bounds

| Bound | Reference | Comments |
| ----- | --------- | -------- |
| 2 | Folklore | See, e.g., Ref (1) |

## Known lower bounds

| Bound | Reference | Comments |
| ----- | --------- | -------- |
| 1 | Folklore | Achieved by constant step sizes. See, e.g., Ref (2). |
| 1.0564 | GSW23 | Nonconstant, fractal pattern |
| 1.178 | GPR23 | Found by computer search on schedules of size 50 |
| 1.271 | AP24 | $\log_2(1+\sqrt{2})$, the "Silver schedule" |

## References

- (1) Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing Company, Incorporated, 1 edition, 2014.
- (2) Dimitri P. Bertsekas. Convex optimization algorithms. 2015.
- (3) Benjamin Grimmer, Kevin Shu, and Alex L. Wang. Accelerated Gradient Descent via Long Steps. arXiv:2309.09961
- (4) Shuvomoy Das Gupta, Bart P.G. Van Parys, and Ernest Ryu. Branch-and-bound performance estimation programming: A unified methodology for constructing optimal optimization methods. Mathematical Programming, 2023
- (5) Jason M Altschuler and Pablo A Parrilo. Acceleration by stepsize hedging: Silver stepsize schedule for smooth convex optimization. Mathematical Programming, pages 1â€“14, 2024.

## Additional comments

For *strongly convex* functions, exponential convergence is generally possible, $O(\log(1/\epsilon))$. But generally speaking, an $O(\epsilon^{-c})$ algorithm for *convex* functions translates to an $O(\kappa^c \log(1/\epsilon))$ algorithm for strongly convex functions, where $\kappa = L / \mu$ is the condition number, $L$ is the Lipschitz constant, and $\mu$ is the minimum curvature.

Many results in literature are stated for Lipschitz functions, with both the step size schedule and the constant prefactor on the convergence rate depending on $L$. It is generally true, though, that these algorithms can be applied to a case with an unknown $L$ and with very mild initial probing the correct asymptotic behavior can be achieved anyway.
